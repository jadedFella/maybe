{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 1]\n",
      " [1 3]]\n",
      "\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "G-mean: 0.75\n",
      "AUC: 0.375\n",
      "True Positive Rate (Sensitivity): 0.75\n",
      "False Alarm Rate: 0.25\n",
      "F-measure: 0.75\n",
      "Overall Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - actual: array, true class labels\n",
    "    - predicted: array, predicted class labels\n",
    "\n",
    "    Returns:\n",
    "    - cm: array, confusion matrix\n",
    "    \"\"\"\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "    for a, p in zip(actual, predicted):\n",
    "        cm[a, p] += 1\n",
    "    return cm\n",
    "\n",
    "def precision(cm):\n",
    "    \"\"\"\n",
    "    Compute precision.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - precision: float\n",
    "    \"\"\"\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "def recall(cm):\n",
    "    \"\"\"\n",
    "    Compute recall.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - recall: float\n",
    "    \"\"\"\n",
    "    tp = cm[1, 1]\n",
    "    fn = cm[1, 0]\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "def g_mean(cm):\n",
    "    \"\"\"\n",
    "    Compute geometric mean.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - g_mean: float\n",
    "    \"\"\"\n",
    "    sensitivity = recall(cm)\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) != 0 else 0\n",
    "    return np.sqrt(sensitivity * specificity)\n",
    "\n",
    "def auc(actual, predicted):\n",
    "    \"\"\"\n",
    "    Compute area under the curve (AUC).\n",
    "\n",
    "    Parameters:\n",
    "    - actual: array, true class labels\n",
    "    - predicted: array, predicted class labels\n",
    "\n",
    "    Returns:\n",
    "    - auc: float\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort(predicted)\n",
    "    actual_sorted = actual[sorted_indices]\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    auc = 0\n",
    "\n",
    "    for i in range(len(sorted_indices)):\n",
    "        if actual_sorted[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "            auc += tp\n",
    "\n",
    "    total_positives = np.sum(actual)\n",
    "    total_negatives = len(actual) - total_positives\n",
    "\n",
    "    if total_positives == 0 or total_negatives == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return auc / (total_positives * total_negatives)\n",
    "\n",
    "def true_positive_rate(cm):\n",
    "    \"\"\"\n",
    "    Compute true positive rate.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - tpr: float\n",
    "    \"\"\"\n",
    "    tp = cm[1, 1]\n",
    "    fn = cm[1, 0]\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "def false_alarm_rate(cm):\n",
    "    \"\"\"\n",
    "    Compute false alarm rate.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - far: float\n",
    "    \"\"\"\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    return fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "\n",
    "def f_measure(cm):\n",
    "    \"\"\"\n",
    "    Compute F-measure.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - f_measure: float\n",
    "    \"\"\"\n",
    "    precision_val = precision(cm)\n",
    "    recall_val = recall(cm)\n",
    "    return 2 * (precision_val * recall_val) / (precision_val + recall_val) if (precision_val + recall_val) != 0 else 0\n",
    "\n",
    "def overall_accuracy(cm):\n",
    "    \"\"\"\n",
    "    Compute overall accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: array, confusion matrix\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: float\n",
    "    \"\"\"\n",
    "    tp = cm[1, 1]\n",
    "    tn = cm[0, 0]\n",
    "    total_samples = np.sum(cm)\n",
    "    return (tp + tn) / total_samples if total_samples != 0 else 0\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    actual_labels = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n",
    "    predicted_labels = np.array([1, 0, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Precision\n",
    "    precision_val = precision(cm)\n",
    "    print(\"\\nPrecision:\", precision_val)\n",
    "\n",
    "    # Recall\n",
    "    recall_val = recall(cm)\n",
    "    print(\"Recall:\", recall_val)\n",
    "\n",
    "    # G-mean\n",
    "    g_mean_val = g_mean(cm)\n",
    "    print(\"G-mean:\", g_mean_val)\n",
    "\n",
    "    # Area under the curve (AUC)\n",
    "    auc_val = auc(actual_labels, predicted_labels)\n",
    "    print(\"AUC:\", auc_val)\n",
    "\n",
    "    # True Positive Rate (Sensitivity)\n",
    "    tpr = true_positive_rate(cm)\n",
    "    print(\"True Positive Rate (Sensitivity):\", tpr)\n",
    "\n",
    "    # False Alarm Rate\n",
    "    far = false_alarm_rate(cm)\n",
    "    print(\"False Alarm Rate:\", far)\n",
    "\n",
    "    # F-measure\n",
    "    f_measure_val = f_measure(cm)\n",
    "    print(\"F-measure:\", f_measure_val)\n",
    "\n",
    "    # Overall Accuracy\n",
    "    accuracy = overall_accuracy(cm)\n",
    "    print(\"Overall Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
